{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing LLMs through Quantization\n",
    "\n",
    "Author - Sri Raghu Malireddi\n",
    "\n",
    "Date - October 3, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Quantization is a process that allows models to be deployed efficiently by reducing the precision of the model's parameters and activations. This is especially important in **Large Language Models (LLMs)**, which contain millions or billions of parameters.\n",
    "\n",
    "LLMs such as GPT family of models, Llama, Mixtral, Phi etc., require large amounts of memory and computational power. Quantization enables us to reduce the size of the model and perform faster computations, all while attempting to maintain the accuracy of the original model.\n",
    "\n",
    "In this notebook, we'll explore the following topics:\n",
    "- Memory Efficiency\n",
    "- Inference Speed-Up\n",
    "- Power Efficiency\n",
    "- Deploying LLMs on Edge Devices\n",
    "- Maintaining Acceptable Accuracy\n",
    "- Scalability and Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Memory Efficiency\n",
    "\n",
    "LLMs contain billions of parameters, which can make them difficult to store and run on standard hardware. Quantization can dramatically reduce the memory footprint of these models by reducing the precision of the model parameters.\n",
    "\n",
    "## Example: Memory Savings with Quantization\n",
    "\n",
    "Let’s take an example of how much memory can be saved by converting from 32-bit floating-point precision (FP32) to 8-bit integer precision (INT8).\n",
    "\n",
    "### Formula for Memory Savings:\n",
    "\n",
    "- FP32: 4 bytes per parameter\n",
    "- INT8: 1 byte per parameter\n",
    "\n",
    "For a model with 1 billion parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage in FP32: 4.0 GB\n",
      "Memory usage in INT8: 1.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Example of memory usage comparison for a 1 billion parameter model\n",
    "parameters = 1e9  # 1 billion parameters\n",
    "\n",
    "# Memory in FP32 (4 bytes per parameter)\n",
    "fp32_memory = parameters * 4 / 1e9  # in GB\n",
    "print(f\"Memory usage in FP32: {fp32_memory} GB\")\n",
    "\n",
    "# Memory in INT8 (1 byte per parameter)\n",
    "int8_memory = parameters * 1 / 1e9  # in GB\n",
    "print(f\"Memory usage in INT8: {int8_memory} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference Speed-Up\n",
    "\n",
    "Quantization not only saves memory but also speeds up inference, which is crucial for real-time applications of LLMs. Integer operations (INT8) are computationally cheaper and faster compared to floating-point operations (FP32).\n",
    "\n",
    "## PyTorch Example: INT8 vs FP32\n",
    "\n",
    "We can use PyTorch to simulate how an LLM would perform with FP32 and quantized INT8 formats.\n",
    "\n",
    "### Convert a Floating-Point Model to INT8 Using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original FP32 Model Size: 32768 bytes\n",
      "Quantized INT8 Model Size: 8192 bytes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "\n",
    "# Simulate a simple FP32 model\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(128, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model_fp32 = SimpleModel()\n",
    "\n",
    "# Quantize the model to INT8\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Compare memory and speed\n",
    "print(f\"Original FP32 Model Size: {model_fp32.fc1.weight.element_size() * model_fp32.fc1.weight.nelement()} bytes\")\n",
    "print(f\"Quantized INT8 Model Size: {model_int8.fc1.weight().element_size() * model_int8.fc1.weight().nelement()} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Power Efficiency\n",
    "\n",
    "One of the lesser-known benefits of quantization is the significant reduction in power consumption, particularly important for deploying LLMs on devices with limited power like mobile phones or IoT devices.\n",
    "\n",
    "Quantized models, particularly INT8, use much less power due to reduced computational overhead and fewer memory access operations. This makes it more feasible to run LLMs on such power-constrained devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deploying LLMs on Edge Devices\n",
    "\n",
    "LLMs are typically deployed in cloud environments due to the immense resources required to run them. However, quantization makes it possible to deploy them on **edge devices**, like smartphones, where memory and computational resources are limited.\n",
    "\n",
    "Quantizing large models can reduce their footprint to the point where they can run efficiently on such devices without compromising user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Maintaining Acceptable Accuracy\n",
    "\n",
    "Quantization has traditionally been viewed as a trade-off between performance and accuracy. However, techniques like **Quantization-Aware Training (QAT)** allow models to retain most of their accuracy while benefiting from the efficiency of quantization.\n",
    "\n",
    "## Example: Quantization-Aware Training (QAT)\n",
    "\n",
    "In QAT, the model is trained with quantization effects simulated during training, allowing it to learn to compensate for the errors introduced by quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Scalability and Cost\n",
    "\n",
    "Quantization allows LLMs to be scaled to even larger sizes without an exponential increase in cost and infrastructure requirements. With quantization, fewer GPUs are needed, inference can happen faster, and memory consumption is much more manageable.\n",
    "\n",
    "In large-scale deployments, these savings translate into significant cost reductions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Types of Quantization - Downcasting\n",
    "\n",
    "## Understanding Floating Point Types: FP32, FP16, and BF16\n",
    "\n",
    "When performing computations in neural networks and other machine learning tasks, the precision of floating-point numbers plays an important role. Different formats like FP32, FP16, and BF16 represent floating-point numbers with varying levels of precision, which impacts performance, memory usage, and the accuracy of computations.\n",
    "\n",
    "### 1. **FP32 (Single-Precision Floating-Point)**\n",
    "\n",
    "- **Bits**: 32 bits\n",
    "- **Structure**: \n",
    "  - 1 bit for sign\n",
    "  - 8 bits for exponent\n",
    "  - 23 bits for the fraction (mantissa)\n",
    "- **Range**: Supports a wide dynamic range with a large number of significant digits.\n",
    "- **Usage**: FP32 is commonly used in most machine learning models as the default precision for training and inference. It provides high accuracy at the cost of memory and computational speed.\n",
    "\n",
    "**Advantages**:\n",
    "  - High precision for both very small and very large numbers.\n",
    "  - Suitable for tasks requiring higher numerical stability.\n",
    "\n",
    "### 2. **FP16 (Half-Precision Floating-Point)**\n",
    "\n",
    "- **Bits**: 16 bits\n",
    "- **Structure**:\n",
    "  - 1 bit for sign\n",
    "  - 5 bits for exponent\n",
    "  - 10 bits for the fraction\n",
    "- **Range**: Reduced dynamic range compared to FP32 due to fewer bits allocated for the exponent and fraction.\n",
    "- **Usage**: FP16 is often used in scenarios where memory and speed are crucial, such as on-device machine learning or inference tasks, but with potential trade-offs in accuracy.\n",
    "\n",
    "**Advantages**:\n",
    "  - Requires half the memory of FP32.\n",
    "  - Faster computations, ideal for low-power devices.\n",
    "\n",
    "**Limitations**:\n",
    "  - Limited precision, which can lead to accuracy loss in complex tasks.\n",
    "  \n",
    "### 3. **BF16 (Bfloat16 Floating-Point)**\n",
    "\n",
    "- **Bits**: 16 bits\n",
    "- **Structure**:\n",
    "  - 1 bit for sign\n",
    "  - 8 bits for exponent (same as FP32)\n",
    "  - 7 bits for the fraction\n",
    "- **Range**: Similar dynamic range to FP32 due to the same 8-bit exponent. However, fewer bits in the fraction reduce the precision of the mantissa.\n",
    "- **Usage**: BF16 is commonly used in large-scale training, particularly on TPUs and GPUs. It offers a good balance between the range of FP32 and the computational efficiency of FP16.\n",
    "\n",
    "**Advantages**:\n",
    "  - Wider dynamic range like FP32 but with less precision.\n",
    "  - More efficient in terms of memory and computation than FP32, without sacrificing as much accuracy as FP16.\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "Here’s a comparison of how these formats allocate bits:\n",
    "\n",
    "![Floating Point Formats](assets/float_types.png)\n",
    "<small><i>Source: https://cerebras.ai/machine-learning/to-bfloat-or-not-to-bfloat-that-is-the-question/</i></small>\n",
    "\n",
    "- FP16 and BF16 use 16 bits but allocate them differently.\n",
    "- FP32 uses more bits for the fraction, allowing for higher precision.\n",
    "- BF16 and FP16 save memory and computation at the cost of precision, but BF16 maintains a broader range due to its 8-bit exponent.\n",
    "\n",
    "### Summary of Key Differences:\n",
    "\n",
    "| Format  | Total Bits | Exponent Bits | Fraction Bits | Use Case                                 |\n",
    "|---------|------------|---------------|---------------|------------------------------------------|\n",
    "| FP32    | 32         | 8             | 23            | High precision, training large models    |\n",
    "| FP16    | 16         | 5             | 10            | Memory-efficient, on-device inference    |\n",
    "| BF16    | 16         | 8             | 7             | Efficient training on TPUs/GPUs          |\n",
    "\n",
    "When choosing between these formats, it's essential to consider the trade-offs between precision and computational efficiency, as some tasks can tolerate less precision, while others require the stability offered by FP32.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Lab - Quantization Datatypes\n",
    "\n",
    "In this lab, lets looks at the basic data types and their memory footprints. PyTorch support various datatypes. For this lab, we will specifically focus on 32 bit and 16 bit Floats, Brain Float 16, 8 bit unsigned and signed integers.\n",
    "\n",
    "Also a simplest form of quantization - downcasting from FP32 to FP16 and BF16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float 32 - finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)\n",
      "Float 16 - finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)\n",
      "Brain Float 16 - finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n",
      "Int 8 - iinfo(min=-128, max=127, dtype=int8)\n",
      "Unsigned Int 8 - iinfo(min=0, max=255, dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "# Lets print some information about these data types\n",
    "print(f\"Float 32 - {torch.finfo(torch.float32)}\")\n",
    "print(f\"Float 16 - {torch.finfo(torch.float16)}\")\n",
    "print(f\"Brain Float 16 - {torch.finfo(torch.bfloat16)}\")\n",
    "print(f\"Int 8 - {torch.iinfo(torch.int8)}\")\n",
    "print(f\"Unsigned Int 8 - {torch.iinfo(torch.uint8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 1/3 # By default, Python stores this value in 64-bit format\n",
    "\n",
    "# Create tensors in various datatypes\n",
    "tensor_fp32 = torch.tensor(value, dtype = torch.float32)\n",
    "tensor_fp16 = torch.tensor(value, dtype = torch.float16)\n",
    "\n",
    "tensor_bf16 = torch.tensor(value, dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64-bit base: \u001b[32m0.333333333333333314829616256247390992939472198486328125\u001b[33m000000\u001b[0m\n",
      "tensor_fp32: \u001b[32m0.3333333432674407958984375\u001b[33m00000000000000000000000000000000000\u001b[0m\n",
      "tensor_fp16: \u001b[32m0.333251953125\u001b[33m000000000000000000000000000000000000000000000000\u001b[0m\n",
      "tensor_bf16: \u001b[32m0.333984375\u001b[33m000000000000000000000000000000000000000000000000000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def highlight_trailing_zeros(value_str):\n",
    "    # Split the string at the first occurrence of trailing zeros\n",
    "    split_index = len(value_str.rstrip('0'))\n",
    "    return f\"\\033[32m{value_str[:split_index]}\\033[33m{value_str[split_index:]}\\033[0m\"\n",
    "\n",
    "\n",
    "print(f\"64-bit base: {highlight_trailing_zeros(format(value, '.60f'))}\")\n",
    "print(f\"tensor_fp32: {highlight_trailing_zeros(format(tensor_fp32.item(), '.60f'))}\")\n",
    "print(f\"tensor_fp16: {highlight_trailing_zeros(format(tensor_fp16.item(), '.60f'))}\")\n",
    "print(f\"tensor_bf16: {highlight_trailing_zeros(format(tensor_bf16.item(), '.60f'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcasting example\n",
    "tensor_fp32 = torch.rand(128, dtype=torch.float32)\n",
    "\n",
    "tensor_fp16 = tensor_fp32.to(dtype=torch.float16)\n",
    "tensor_bf16 = tensor_fp32.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp32: \u001b[32m39.338592529296875\u001b[33m00000\u001b[0m\n",
      "fp16: \u001b[32m39.34375\u001b[33m000000000000000\u001b[0m\n",
      "bf16: \u001b[32m39.25\u001b[33m000000000000000000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Let's perform a simple dot product and see the difference in values\n",
    "\n",
    "m_float32 = torch.dot(tensor_fp32, tensor_fp32)\n",
    "m_float16 = torch.dot(tensor_fp16, tensor_fp16)\n",
    "m_bfloat16 = torch.dot(tensor_bf16, tensor_bf16)\n",
    "\n",
    "print(f\"fp32: {highlight_trailing_zeros(format(m_float32, '.20f'))}\")\n",
    "print(f\"fp16: {highlight_trailing_zeros(format(m_float16, '.20f'))}\")\n",
    "print(f\"bf16: {highlight_trailing_zeros(format(m_bfloat16, '.20f'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Lab - Quantization - Downcasting\n",
    "\n",
    "In this lab, we will load a model and downcast it into different datatypes. For each datatype, we will evaluate its accuracy on the test dataset. \n",
    "\n",
    "<small><i>Model credits: https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion</i></small>\n",
    "\n",
    "<small><i>Data credits: https://huggingface.co/datasets/dair-ai/emotion</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sri/miniconda3/envs/llmq/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'bhadresh-savani/distilbert-base-uncased-emotion'\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"dair-ai/emotion\", \"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original model accuracy: 92.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the original model\n",
    "def evaluate(model, ds, batch_size=64, device='cuda'):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    dataloader = DataLoader(ds['test'], batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            tokenized = tokenizer(batch['text'], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            input_ids = tokenized.input_ids.to(device)\n",
    "            attention_mask = tokenized.attention_mask.to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "original_accuracy = evaluate(model, ds)\n",
    "print(f'\\nOriginal model accuracy: {original_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████| 32/32 [00:03<00:00, 10.22it/s]\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 25.15it/s]\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FP32 model accuracy: 92.70%\n",
      "FP16 model accuracy: 92.70%\n",
      "BF16 model accuracy: 92.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets create different variations of downcasted model\n",
    "def create_model(datatype=torch.float16):\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "    model.to(datatype)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model_fp32 = create_model(torch.float32)\n",
    "model_fp16 = create_model(torch.float16)\n",
    "model_bf16 = create_model(torch.bfloat16)\n",
    "\n",
    "accuracy_fp32 = evaluate(model_fp32, ds)\n",
    "accuracy_fp16 = evaluate(model_fp16, ds)\n",
    "accuracy_bf16 = evaluate(model_bf16, ds)\n",
    "\n",
    "print(f'\\nFP32 model accuracy: {accuracy_fp32 * 100:.2f}%')\n",
    "print(f'FP16 model accuracy: {accuracy_fp16 * 100:.2f}%')\n",
    "print(f'BF16 model accuracy: {accuracy_bf16 * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: 255.4287338256836 MB\n",
      "FP16: 127.7163200378418 MB\n",
      "BF16: 127.7163200378418 MB\n"
     ]
    }
   ],
   "source": [
    "# Lets check these models runtime memory footprint\n",
    "\n",
    "print(f\"FP32: {model_fp32.get_memory_footprint() / (1024 * 1024)} MB\")\n",
    "print(f\"FP16: {model_fp16.get_memory_footprint() / (1024 * 1024)} MB\")\n",
    "print(f\"BF16: {model_bf16.get_memory_footprint() / (1024 * 1024)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above results, there is no notable degradation in performance between FP32, FP16 and BF16 datatypes. \n",
    "\n",
    "NOTE - Observe the speed-ups achieved with FP16 inference on the evaluation dataset.\n",
    "\n",
    "From this we can conclude that, just by deploying the model in FP16 format, we will be able to get `50 %` reduction in model size and `> 2x` speed-up in inference. \n",
    "\n",
    "Now let's go a step further and perform integer quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Quantization - Integer weight-only\n",
    "\n",
    "In this section, we explore **Integer Quantization**, a method where floating-point values are mapped to integers, reducing memory footprint and improving inference speed. Integer quantization is especially popular for deployment in edge devices and hardware accelerators like TPUs or GPUs.\n",
    "\n",
    "## Post-Training Integer Quantization\n",
    "\n",
    "**Post-Training Quantization** is applied after the model has been trained, without requiring model retraining. Weights and activations are mapped from floating-point (usually FP32) to integers (e.g., INT8) using a **scale** and a **zero-point**.\n",
    "\n",
    "### Scale and Zero-Point\n",
    "\n",
    "- **Scale**: A multiplicative factor that scales the floating-point range to the integer range.\n",
    "- **Zero-point**: An integer value that maps floating-point zero to an integer value, compensating for any offsets in the original range.\n",
    "\n",
    "For example:\n",
    "- **Scale**: `0.1`\n",
    "- **Zero-point**: `0`\n",
    "- A floating-point value of `0.5` will be quantized as `int8(0.5 / 0.1) = 5`.\n",
    "\n",
    "Post-training integer quantization does not require retraining, making it easy to implement. It is particularly useful when speed and memory savings are more important than exact precision.\n",
    "\n",
    "## Per-Tensor Quantization\n",
    "\n",
    "In **Per-Tensor Quantization**, a **single scale and zero-point** are used for the entire tensor (e.g., a weight matrix or activation map). \n",
    "\n",
    "- It’s faster because only one scale and zero-point are used for the whole tensor.\n",
    "- It works well when the tensor data has a uniform distribution.\n",
    "- **Drawback**: It can lead to reduced precision when the tensor's values vary significantly.\n",
    "\n",
    "## Per-Channel Quantization\n",
    "\n",
    "**Per-Channel Quantization** applies different quantization parameters (scale and zero-point) to **individual channels** of the tensor, which is especially useful in convolutional layers of neural networks.\n",
    "\n",
    "- Each channel (e.g., weight filter in a CNN) has its own scale and zero-point.\n",
    "- This leads to better accuracy compared to per-tensor quantization, especially when different channels have varying data distributions.\n",
    "- **Drawback**: Slightly more computational overhead because each channel needs its own scale and zero-point.\n",
    "\n",
    "## Symmetric vs. Asymmetric Quantization\n",
    "\n",
    "- **Symmetric Quantization**: The zero-point is fixed at `0`, meaning the range is symmetric around zero. This simplifies calculations and is often used in models that are well-centered around zero.\n",
    "- **Asymmetric Quantization**: Allows for a non-zero zero-point, which provides more flexibility in cases where the data distribution isn’t centered around zero.\n",
    "\n",
    "---\n",
    "\n",
    "# Example of Integer Quantization in PyTorch\n",
    "\n",
    "Let's see how we can apply **integer quantization** to a PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from FP32 quantized model: tensor([[ 0.0033,  0.4057, -0.1154,  0.2642, -0.0642, -0.2054, -0.1421,  0.3769,\n",
      "         -0.0029,  0.1919,  0.4293,  0.0696,  0.1322,  0.4005,  0.1255, -0.1972,\n",
      "          0.1423,  0.3478,  0.1480, -0.1819,  0.1038, -0.0499,  0.0911, -0.2550,\n",
      "          0.2613,  0.2535, -0.0996, -0.2911, -0.8366,  0.0810,  0.5041,  0.2107]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Output from INT8 quantized model: tensor([[ 0.0087,  0.4169, -0.1180,  0.2583, -0.0688, -0.2049, -0.1453,  0.3776,\n",
      "          0.0034,  0.1910,  0.4294,  0.0789,  0.1383,  0.3942,  0.1181, -0.1975,\n",
      "          0.1439,  0.3552,  0.1535, -0.1834,  0.0965, -0.0472,  0.0986, -0.2555,\n",
      "          0.2624,  0.2425, -0.1019, -0.2953, -0.8378,  0.0809,  0.5058,  0.2076]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "\n",
    "# Example: Simulating an INT8 model from an FP32 model using Post-Training Quantization\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(128, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Create an FP32 model\n",
    "model_fp32 = SimpleModel()\n",
    "\n",
    "# Apply dynamic quantization to convert it to INT8\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Test with some random input\n",
    "input_fp32 = torch.randn(1, 128)\n",
    "output_fp32 = model_fp32(input_fp32)\n",
    "output_int8 = model_int8(input_fp32)\n",
    "\n",
    "print(f\"Output from FP32 quantized model: {output_fp32}\")\n",
    "print(f\"Output from INT8 quantized model: {output_int8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Lab - Integer Quantization in LLMs\n",
    "\n",
    "When it comes to LLMs, **BitsAndBytes** is the easiest option for quantizing a model to 8-bit and 4-bit precision. For 8-bit quantization, it handles outliers by multiplying them in FP16 with non-outliers in INT8, converting the non-outlier values back to FP16, and then summing them up to return the weights in FP16. This approach minimizes the negative impact outlier values can have on a model’s performance. Meanwhile, 4-bit quantization goes a step further in compressing the model, commonly used alongside **QLoRA** for fine-tuning quantized large language models (LLMs).\n",
    "\n",
    "To learn more about the BitsAndBytes, check this link - https://github.com/bitsandbytes-foundation/bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftConfig, get_peft_model\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_8bit = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "model_8bit.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_4bit = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "model_4bit.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 bit: 86.6538200378418 MB\n",
      "4 bit: 66.1225700378418 MB\n"
     ]
    }
   ],
   "source": [
    "# Lets check these models runtime memory footprint\n",
    "\n",
    "print(f\"8 bit: {model_8bit.get_memory_footprint() / (1024 * 1024)} MB\")\n",
    "print(f\"4 bit: {model_4bit.get_memory_footprint() / (1024 * 1024)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  7.33it/s]\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 21.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 bit model accuracy: 28.40%\n",
      "4 bit model accuracy: 92.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us choose 4-bit model and check its accuracy\n",
    "accuracy_8bit = evaluate(model_8bit, ds)\n",
    "accuracy_4bit = evaluate(model_4bit, ds)\n",
    "\n",
    "print(f'\\n8 bit model accuracy: {accuracy_8bit * 100:.2f}%')\n",
    "print(f'4 bit model accuracy: {accuracy_4bit * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific example, the model we used was able to work with decent accuracy when quantized to 4-bit. In practice, we perform more rigorous evaluation and if the 4-bit model is up-to the desired quality standards, we should be able to productionize it without further training.\n",
    "\n",
    "In most cases, we will need to perform quantization-aware-training or PEFT (parameter-efficient fine tuning) to reach decent accuracy numbers. As you can see, the 8-bit model's accuracy significantly dropped, we can perform PEFT to improve its accuracy. For a lab on this please refer to the notebook - [PEFT-DistilBert.ipynb](./PEFT-DistilBert.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "To learn more about the quantization in LLMs and follow some latest techniques like GPTQ and AWQ, please refer to the official HuggingFace documentation - [HuggingFace Quantization](https://huggingface.co/docs/transformers/v4.45.1/quantization/overview). \n",
    "\n",
    "HuggingFace's Transformers library does support majority of the latest quantization techniques and the following table reflects their current support matrix - \n",
    "\n",
    "![Support Matrix](assets/hf_quant_support_matrix.png)\n",
    "\n",
    "<small><i>Source: https://huggingface.co/docs/transformers/v4.45.1/quantization/overview</i></small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
